# Natural Language Processing with Attention Models

* [Neural Machine Translation](https://github.com/msankar/natural_language_processing/tree/main/nlp_with_attention_models/week1-neural_machine_translation)
* [Text Summarization](https://github.com/msankar/natural_language_processing/tree/main/nlp_with_attention_models/week2-text_summarization)
* [Question Answering](https://github.com/msankar/natural_language_processing/tree/main/nlp_with_attention_models/week3-question_answering)
* [Chatbot](https://github.com/msankar/natural_language_processing/tree/main/nlp_with_attention_models/week4-chatbot)


REFERENCES:
This course drew from the following resources:
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)
- Reformer: The Efficient Transformer (Kitaev et al, 2020)
- Attention Is All You Need (Vaswani et al, 2017)
-â€‹ Deep contextualized word representations (Peters et al, 2018)
- The Illustrated Transformer (Alammar, 2018)
- The Illustrated GPT-2 (Visualizing Transformer Language Models) (Alammar, 2019)
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)
- How GPT3 Works - Visualizations and Animations (Alammar, 2020)
