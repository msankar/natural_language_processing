# Autocomplete and Language Models

Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter

## Learning Objectives
* Conditional probabilities
* Text pre-processing
* Language modeling
* Perplexity
* K-smoothing
* N-grams
* Backoff
* Tokenization
